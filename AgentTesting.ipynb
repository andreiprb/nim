{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "7b4c3b0961ac3b1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T12:09:35.088711Z",
     "start_time": "2025-04-25T12:09:35.067881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "id": "f80b91b841340426",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T12:09:35.145224Z",
     "start_time": "2025-04-25T12:09:35.140937Z"
    }
   },
   "source": [
    "from nim.Nim import Nim\n",
    "\n",
    "from agents.Minimax.MinimaxAgentV1 import MinimaxAgentV1\n",
    "from agents.Minimax.MinimaxAgentV2 import MinimaxAgentV2\n",
    "\n",
    "from agents.QLearning.QLearningAgentV1 import QLearningAgentV1"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ParametrizedAgent class (helper)",
   "id": "8b6f4ab38e9d516"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T12:09:35.151796Z",
     "start_time": "2025-04-25T12:09:35.150101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ParametrizedAgent:\n",
    "    def __init__(self, agent_class, *param_names):\n",
    "        self.agent_class = agent_class\n",
    "        self.param_names = param_names\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        params = {k: v for k, v in kwargs.items() if k in self.param_names}\n",
    "        return self.agent_class(**params)\n",
    "\n",
    "# AGENT = ParametrizedAgent(MinimaxAgentV1, \"misere\", \"max_depth\")\n",
    "# AGENT = ParametrizedAgent(MinimaxAgentV2, \"misere\", \"max_depth\")\n",
    "AGENT = ParametrizedAgent(QLearningAgentV1, \"misere\", \"initial_piles\", \"alpha\", \"epsilon\", \"gamma\", \"decay_rate\")"
   ],
   "id": "abe2fd3436a584e4",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Constants",
   "id": "6469a4493d07d6a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T12:09:35.160512Z",
     "start_time": "2025-04-25T12:09:35.159178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Minimax Agents\n",
    "MAX_DEPTH = 1\n",
    "\n",
    "# QLearning Agents\n",
    "ALPHA = 0.5\n",
    "EPSILLON = 0.1\n",
    "GAMMA = 0.9\n",
    "DECAY_RATE = 0.9999"
   ],
   "id": "2734ef9cdbee5047",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Assert function (helper)",
   "id": "9b3d188180eca026"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T12:09:35.169355Z",
     "start_time": "2025-04-25T12:09:35.167447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hard_assert(_misere, _initial_piles, _winner):\n",
    "    piles = np.array(_initial_piles)\n",
    "\n",
    "    if _misere:\n",
    "        if np.all(piles <= 1):\n",
    "            assert _winner == np.sum(piles) % 2, \"Misere Nim - Corner Case\"\n",
    "\n",
    "    else:\n",
    "        assert _winner == int(np.bitwise_xor.reduce(piles) != _misere), f\"{'Misere' if misere else 'Normal'} Nim - All Cases\""
   ],
   "id": "c3957002d3e11593",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Game Setup",
   "id": "caee15f67b9ae8b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T12:11:17.773435Z",
     "start_time": "2025-04-25T12:11:17.770854Z"
    }
   },
   "cell_type": "code",
   "source": [
    "misere = True\n",
    "initial_piles = [1, 0, 3, 1]\n",
    "\n",
    "player1 = AGENT(misere=misere, initial_piles=initial_piles, max_depth=MAX_DEPTH)\n",
    "player2 = AGENT(misere=misere, initial_piles=initial_piles, max_depth=MAX_DEPTH)\n",
    "\n",
    "game = Nim(\n",
    "    initial_piles=initial_piles,\n",
    "    misere=misere\n",
    ")"
   ],
   "id": "12377bf62677145b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values loaded from savedAgents/qlearning-1-0-3-1-True.json\n",
      "Q-values loaded from savedAgents/qlearning-1-0-3-1-True.json\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## One game test (verbose)",
   "id": "8caf6e9c1c377962"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T12:11:20.532005Z",
     "start_time": "2025-04-25T12:11:20.529738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "winner = game.play(\n",
    "    player1=player1,\n",
    "    player2=player2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "hard_assert(misere, initial_piles, winner)"
   ],
   "id": "f97d02e6c6f50f43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Misere game\n",
      "Piles: [1, 0, 3, 1]\n",
      "Player 1 (QLearningAgent agent) takes 1 from pile 2\n",
      "Piles: [1, 0, 2, 1]\n",
      "Player 2 (MinimaxV2 agent) takes 1 from pile 2\n",
      "Piles: [1, 0, 1, 1]\n",
      "Player 1 (QLearningAgent agent) takes 1 from pile 3\n",
      "Piles: [1, 0, 1, 0]\n",
      "Player 2 (MinimaxV2 agent) takes 1 from pile 0\n",
      "Piles: [0, 0, 1, 0]\n",
      "Player 1 (QLearningAgent agent) takes 1 from pile 2\n",
      "Player 2 (MinimaxV2 agent) wins!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10k game test",
   "id": "dfa6aeed09cf028d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T12:11:23.174797Z",
     "start_time": "2025-04-25T12:11:23.153366Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wins = [0, 0]\n",
    "\n",
    "for _ in range(10000):\n",
    "    winner = game.play(\n",
    "        player1=player1,\n",
    "        player2=player2,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    hard_assert(misere, initial_piles, winner)\n",
    "    wins[winner] += 1"
   ],
   "id": "e2a35608cd8aa9d5",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10k random game test (Misere)",
   "id": "38122e9bb8d5c6dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T12:10:53.240979Z",
     "start_time": "2025-04-25T12:10:25.587502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wins = [0, 0]\n",
    "\n",
    "for _ in tqdm(range(10000)):\n",
    "    misere = np.random.choice([True, False])\n",
    "    initial_piles = list(np.random.randint(1, 255, size=8))\n",
    "\n",
    "    player1 = AGENT(misere=misere, initial_piles=initial_piles, max_depth=MAX_DEPTH)\n",
    "    player2 = AGENT(misere=misere, initial_piles=initial_piles, max_depth=MAX_DEPTH)\n",
    "\n",
    "    game = Nim(\n",
    "        initial_piles=initial_piles,\n",
    "        misere=misere\n",
    "    )\n",
    "\n",
    "    winner = game.play(\n",
    "        player1=player1,\n",
    "        player2=player2,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    hard_assert(misere, initial_piles, winner)\n",
    "    wins[winner] += 1"
   ],
   "id": "b1cb07b39efc962a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Q-Learning agent for 50000 episodes...\n",
      "Episode 1000/50000 - Epsilon: 0.090483\n",
      "Episode 2000/50000 - Epsilon: 0.081872\n",
      "Episode 3000/50000 - Epsilon: 0.074081\n",
      "Episode 4000/50000 - Epsilon: 0.067031\n",
      "Episode 5000/50000 - Epsilon: 0.060652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10000 [00:27<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m misere \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice([\u001B[38;5;28;01mTrue\u001B[39;00m, \u001B[38;5;28;01mFalse\u001B[39;00m])\n\u001B[1;32m      5\u001B[0m initial_piles \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m255\u001B[39m, size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m8\u001B[39m))\n\u001B[0;32m----> 7\u001B[0m player1 \u001B[38;5;241m=\u001B[39m \u001B[43mAGENT\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmisere\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmisere\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_piles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitial_piles\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_depth\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mMAX_DEPTH\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m player2 \u001B[38;5;241m=\u001B[39m AGENT(misere\u001B[38;5;241m=\u001B[39mmisere, initial_piles\u001B[38;5;241m=\u001B[39minitial_piles, max_depth\u001B[38;5;241m=\u001B[39mMAX_DEPTH)\n\u001B[1;32m     10\u001B[0m game \u001B[38;5;241m=\u001B[39m Nim(\n\u001B[1;32m     11\u001B[0m     initial_piles\u001B[38;5;241m=\u001B[39minitial_piles,\n\u001B[1;32m     12\u001B[0m     misere\u001B[38;5;241m=\u001B[39mmisere\n\u001B[1;32m     13\u001B[0m )\n",
      "Cell \u001B[0;32mIn[3], line 8\u001B[0m, in \u001B[0;36mParametrizedAgent.__call__\u001B[0;34m(self, **kwargs)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m      7\u001B[0m     params \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparam_names}\n\u001B[0;32m----> 8\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43magent_class\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/agents/QLearning/QLearningAgentV1.py:28\u001B[0m, in \u001B[0;36mQLearningAgentV1.__init__\u001B[0;34m(self, misere, initial_piles, alpha, epsilon, gamma, decay_rate)\u001B[0m\n\u001B[1;32m     26\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mload_q_values()\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 28\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/agents/QLearning/QLearningAgentV1.py:163\u001B[0m, in \u001B[0;36mQLearningAgentV1.train\u001B[0;34m(self, num_episodes)\u001B[0m\n\u001B[1;32m    160\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m j \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(episode_history) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    161\u001B[0m         transition[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreward\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma \u001B[38;5;241m*\u001B[39m episode_history[j \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreward\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m--> 163\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mupdate_q_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    164\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtransition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mstate\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    165\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtransition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43maction\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtransition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mreward\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    167\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtransition\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnext_state\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[1;32m    168\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    170\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepsilon \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecay_rate\n\u001B[1;32m    172\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (i \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1000\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/agents/QLearning/QLearningAgentV1.py:64\u001B[0m, in \u001B[0;36mQLearningAgentV1.update_q_value\u001B[0;34m(self, state, action, reward, next_state)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mupdate_q_value\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, action, reward, next_state):\n\u001B[1;32m     62\u001B[0m     old_q \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_q_value(state, action)\n\u001B[0;32m---> 64\u001B[0m     max_future_q \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbest_future_reward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m     new_q \u001B[38;5;241m=\u001B[39m old_q \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha \u001B[38;5;241m*\u001B[39m (reward \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma \u001B[38;5;241m*\u001B[39m max_future_q \u001B[38;5;241m-\u001B[39m old_q)\n\u001B[1;32m     68\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq[(\u001B[38;5;28mtuple\u001B[39m(state), action)] \u001B[38;5;241m=\u001B[39m new_q\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/agents/QLearning/QLearningAgentV1.py:78\u001B[0m, in \u001B[0;36mQLearningAgentV1.best_future_reward\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m     76\u001B[0m max_q \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-inf\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m action \u001B[38;5;129;01min\u001B[39;00m actions:\n\u001B[0;32m---> 78\u001B[0m     q_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_q_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m     max_q \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(max_q, q_value)\n\u001B[1;32m     81\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m max_q \u001B[38;5;28;01mif\u001B[39;00m max_q \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m-inf\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/agents/QLearning/QLearningAgentV1.py:58\u001B[0m, in \u001B[0;36mQLearningAgentV1.get_q_value\u001B[0;34m(self, state, action)\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mError loading Q-values: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     56\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n\u001B[0;32m---> 58\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mget_q_value\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, action):\n\u001B[1;32m     59\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mq\u001B[38;5;241m.\u001B[39mget((\u001B[38;5;28mtuple\u001B[39m(state), action), \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mupdate_q_value\u001B[39m(\u001B[38;5;28mself\u001B[39m, state, action, reward, next_state):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T12:09:36.151995Z",
     "start_time": "2025-04-25T12:01:14.723365Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "da7ed4512a7f5bca",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
