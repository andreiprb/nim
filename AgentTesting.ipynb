{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Imports",
   "id": "7b4c3b0961ac3b1e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T18:59:02.963136Z",
     "start_time": "2025-04-25T18:59:02.960391Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ],
   "id": "f80b91b841340426",
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-25T18:59:03.170502Z",
     "start_time": "2025-04-25T18:59:03.166600Z"
    }
   },
   "source": [
    "from nim.Nim import Nim\n",
    "\n",
    "from agents.Minimax.MinimaxAgentV1 import MinimaxAgentV1\n",
    "from agents.Minimax.MinimaxAgentV2 import MinimaxAgentV2\n",
    "\n",
    "from agents.QLearning.QLearningAgentV1 import QLearningAgentV1\n",
    "from agents.QLearning.QLearningAgentV2 import QLearningAgentV2"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Constants",
   "id": "6469a4493d07d6a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T19:00:33.920510Z",
     "start_time": "2025-04-25T19:00:33.916379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Test parameters\n",
    "EPISODES = 10000\n",
    "\n",
    "# Game parameters\n",
    "MISERE = False\n",
    "MAX_PILE = 127\n",
    "PILE_COUNT = 4\n",
    "MAX_PILES = [MAX_PILE] * PILE_COUNT\n",
    "INITIAL_PILES = np.random.randint(1, MAX_PILE, size=PILE_COUNT)\n",
    "\n",
    "# Minimax parameters\n",
    "MAX_DEPTH = 3\n",
    "\n",
    "# QLearning Parameters\n",
    "ALPHA = 0.5\n",
    "EPSILLON = 0.1\n",
    "GAMMA = 0.9\n",
    "DECAY_RATE = 0.9999"
   ],
   "id": "2734ef9cdbee5047",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ParametrizedAgent class (helper)",
   "id": "8b6f4ab38e9d516"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T19:00:34.176048Z",
     "start_time": "2025-04-25T19:00:34.172482Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ParametrizedAgent:\n",
    "    def __init__(self, agent_class, *param_names):\n",
    "        self.agent_class = agent_class\n",
    "        self.param_names = param_names\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        params = {k: v for k, v in kwargs.items() if k in self.param_names}\n",
    "        return self.agent_class(**params)"
   ],
   "id": "abe2fd3436a584e4",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Agent selection",
   "id": "f44e6e251df13c61"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T19:00:34.407255Z",
     "start_time": "2025-04-25T19:00:34.404824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# AGENT = ParametrizedAgent(MinimaxAgentV1, \"misere\", \"max_depth\")\n",
    "AGENT = ParametrizedAgent(MinimaxAgentV2, \"misere\", \"max_depth\")\n",
    "\n",
    "# AGENT = ParametrizedAgent(QLearningAgentV1, \"misere\", \"max_piles\", \"alpha\", \"epsilon\", \"gamma\", \"decay_rate\")\n",
    "# AGENT = ParametrizedAgent(QLearningAgentV2, \"misere\", \"max_piles\", \"alpha\", \"epsilon\", \"gamma\", \"decay_rate\")"
   ],
   "id": "6ccd39f2197b95ea",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Assert function (helper)",
   "id": "9b3d188180eca026"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T19:00:37.552074Z",
     "start_time": "2025-04-25T19:00:37.547321Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def hard_assert(_misere, _initial_piles, _winner):\n",
    "    piles = np.array(_initial_piles)\n",
    "\n",
    "    if _misere:\n",
    "        if np.all(piles <= 1):\n",
    "            assert _winner == np.sum(piles) % 2, \"Misere Nim - Corner Case\"\n",
    "\n",
    "    else:\n",
    "        assert _winner == int(np.bitwise_xor.reduce(piles) != _misere), f\"{'Misere' if _misere else 'Normal'} Nim - All Cases\"\n",
    "\n",
    "def soft_assert(_misere, _initial_piles, _winner, _wins):\n",
    "    piles = np.array(_initial_piles)\n",
    "    nim_sum = np.bitwise_xor.reduce(piles)\n",
    "\n",
    "    game_type = \"M\" if _misere else \"N\"\n",
    "    start_sum = \"=0\" if int(nim_sum == 0) else \">0\"\n",
    "    p_who_won = \"P\" + str(_winner + 1)\n",
    "\n",
    "    _wins[f\"{game_type}_{start_sum}_{p_who_won}\"] = _wins.get(f\"{game_type}_{start_sum}_{p_who_won}\", 0) + 1"
   ],
   "id": "c3957002d3e11593",
   "outputs": [],
   "execution_count": 63
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Agent Setup",
   "id": "caee15f67b9ae8b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T19:00:38.341220Z",
     "start_time": "2025-04-25T19:00:38.337663Z"
    }
   },
   "cell_type": "code",
   "source": [
    "misereAgent = AGENT(misere=True, max_piles=MAX_PILES, max_depth=MAX_DEPTH)\n",
    "normalAgent = AGENT(misere=False, max_piles=MAX_PILES, max_depth=MAX_DEPTH)"
   ],
   "id": "12377bf62677145b",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# One game demo",
   "id": "8caf6e9c1c377962"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T19:00:39.216337Z",
     "start_time": "2025-04-25T19:00:39.095958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "game = Nim(\n",
    "    initial_piles=INITIAL_PILES,\n",
    "    misere=MISERE\n",
    ")\n",
    "\n",
    "winner = game.play(\n",
    "    player1=misereAgent if MISERE else normalAgent,\n",
    "    player2=misereAgent if MISERE else normalAgent\n",
    ")"
   ],
   "id": "f97d02e6c6f50f43",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal game\n",
      "Piles: [103  52  93  15]\n",
      "Player 1 (MinimaxV2) takes 1 from pile 3\n",
      "Agent 1 (MinimaxV2) NPM: 18096.00\n",
      "Agent 2 (MinimaxV2) NPM: 18096.00\n",
      "Piles: [103  52  93  14]\n",
      "Player 2 (MinimaxV2) takes 103 from pile 0\n",
      "Agent 1 (MinimaxV2) NPM: 18088.00\n",
      "Agent 2 (MinimaxV2) NPM: 18088.00\n",
      "Piles: [ 0 52 93 14]\n",
      "Player 1 (MinimaxV2) takes 35 from pile 2\n",
      "Agent 1 (MinimaxV2) NPM: 12509.67\n",
      "Agent 2 (MinimaxV2) NPM: 12509.67\n",
      "Piles: [ 0 52 58 14]\n",
      "Player 2 (MinimaxV2) takes 58 from pile 2\n",
      "Agent 1 (MinimaxV2) NPM: 9901.00\n",
      "Agent 2 (MinimaxV2) NPM: 9901.00\n",
      "Piles: [ 0 52  0 14]\n",
      "Player 1 (MinimaxV2) takes 38 from pile 1\n",
      "Agent 1 (MinimaxV2) NPM: 7928.80\n",
      "Agent 2 (MinimaxV2) NPM: 7928.80\n",
      "Piles: [ 0 14  0 14]\n",
      "Player 2 (MinimaxV2) takes 14 from pile 1\n",
      "Agent 1 (MinimaxV2) NPM: 6620.50\n",
      "Agent 2 (MinimaxV2) NPM: 6620.50\n",
      "Piles: [ 0  0  0 14]\n",
      "Player 1 (MinimaxV2) takes 13 from pile 3\n",
      "Agent 1 (MinimaxV2) NPM: 5675.00\n",
      "Agent 2 (MinimaxV2) NPM: 5675.00\n",
      "Piles: [0 0 0 1]\n",
      "Player 2 (MinimaxV2) takes 1 from pile 3\n",
      "Player 2 (MinimaxV2) wins!\n",
      "\n",
      "Agent 1 (MinimaxV2) NPM: 4965.88\n",
      "Agent 2 (MinimaxV2) NPM: 4965.88\n"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 10k random game test (Misere)",
   "id": "38122e9bb8d5c6dd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T19:00:54.004839Z",
     "start_time": "2025-04-25T19:00:42.712631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "wins = {}\n",
    "\n",
    "mean_nodes = 0\n",
    "\n",
    "for _ in tqdm(range(EPISODES)):\n",
    "    misere = np.random.choice([True, False])\n",
    "    initial_piles = list(np.random.randint(1, MAX_PILE, size=PILE_COUNT))\n",
    "\n",
    "    game = Nim(\n",
    "        initial_piles=initial_piles,\n",
    "        misere=misere\n",
    "    )\n",
    "\n",
    "    winner, mn = game.play(\n",
    "        player1=misereAgent if MISERE else normalAgent,\n",
    "        player2=misereAgent if MISERE else normalAgent,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    mean_nodes += mn\n",
    "\n",
    "    # hard_assert(misere, initial_piles, winner)\n",
    "    soft_assert(misere, initial_piles, winner, wins)\n",
    "\n",
    "for k, v in wins.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "if mean_nodes:\n",
    "    mean_nodes /= EPISODES\n",
    "    print(f\"\\nMean nodes: {mean_nodes:.2f}\")"
   ],
   "id": "b1cb07b39efc962a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 291/10000 [00:11<06:15, 25.85it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[66], line 14\u001B[0m\n\u001B[1;32m      7\u001B[0m initial_piles \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m1\u001B[39m, MAX_PILE, size\u001B[38;5;241m=\u001B[39mPILE_COUNT))\n\u001B[1;32m      9\u001B[0m game \u001B[38;5;241m=\u001B[39m Nim(\n\u001B[1;32m     10\u001B[0m     initial_piles\u001B[38;5;241m=\u001B[39minitial_piles,\n\u001B[1;32m     11\u001B[0m     misere\u001B[38;5;241m=\u001B[39mmisere\n\u001B[1;32m     12\u001B[0m )\n\u001B[0;32m---> 14\u001B[0m winner, mn \u001B[38;5;241m=\u001B[39m \u001B[43mgame\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplay\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[43m    \u001B[49m\u001B[43mplayer1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmisereAgent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mMISERE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnormalAgent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     16\u001B[0m \u001B[43m    \u001B[49m\u001B[43mplayer2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmisereAgent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mMISERE\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mnormalAgent\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     17\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[1;32m     18\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m mean_nodes \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m mn\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# hard_assert(misere, initial_piles, winner)\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/nim/Nim.py:45\u001B[0m, in \u001B[0;36mNim.play\u001B[0;34m(self, player1, player2, verbose)\u001B[0m\n\u001B[1;32m     42\u001B[0m current_player \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mplayer\n\u001B[1;32m     43\u001B[0m current_agent \u001B[38;5;241m=\u001B[39m players[current_player]\n\u001B[0;32m---> 45\u001B[0m pile, count \u001B[38;5;241m=\u001B[39m \u001B[43mcurrent_agent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoose_action\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpiles\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verbose:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlayer \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mint\u001B[39m(current_player)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_agent\u001B[38;5;241m.\u001B[39mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) takes \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcount\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m from pile \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpile\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/agents/Minimax/MinimaxAgentV1.py:20\u001B[0m, in \u001B[0;36mMinimaxAgentV1.choose_action\u001B[0;34m(self, state)\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mchoose_action\u001B[39m(\u001B[38;5;28mself\u001B[39m, state):\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmoves_count \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m---> 20\u001B[0m     _, best_action \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_minimax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m-inf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43minf\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmean_nodes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnodes_explored \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmoves_count\n\u001B[1;32m     24\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m best_action\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/agents/Minimax/MinimaxAgentV2.py:42\u001B[0m, in \u001B[0;36mMinimaxAgentV2._minimax\u001B[0;34m(self, state, player, alpha, beta, depth)\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     40\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m depth \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault, action\n\u001B[0;32m---> 42\u001B[0m new_value, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_minimax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_value \u001B[38;5;241m>\u001B[39m value:\n\u001B[1;32m     44\u001B[0m     value \u001B[38;5;241m=\u001B[39m new_value\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/agents/Minimax/MinimaxAgentV2.py:66\u001B[0m, in \u001B[0;36mMinimaxAgentV2._minimax\u001B[0;34m(self, state, player, alpha, beta, depth)\u001B[0m\n\u001B[1;32m     63\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault \u001B[38;5;241m-\u001B[39m depth, action\n\u001B[0;32m---> 66\u001B[0m new_value, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_minimax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_value \u001B[38;5;241m<\u001B[39m value:\n\u001B[1;32m     68\u001B[0m     value \u001B[38;5;241m=\u001B[39m new_value\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/agents/Minimax/MinimaxAgentV2.py:42\u001B[0m, in \u001B[0;36mMinimaxAgentV2._minimax\u001B[0;34m(self, state, player, alpha, beta, depth)\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     40\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m depth \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault, action\n\u001B[0;32m---> 42\u001B[0m new_value, _ \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_minimax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbeta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdepth\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m new_value \u001B[38;5;241m>\u001B[39m value:\n\u001B[1;32m     44\u001B[0m     value \u001B[38;5;241m=\u001B[39m new_value\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/agents/Minimax/MinimaxAgentV2.py:19\u001B[0m, in \u001B[0;36mMinimaxAgentV2._minimax\u001B[0;34m(self, state, player, alpha, beta, depth)\u001B[0m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m sign \u001B[38;5;241m*\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdefault \u001B[38;5;241m-\u001B[39m depth), \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_depth \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m depth \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_depth:\n\u001B[0;32m---> 19\u001B[0m     heuristic_score \u001B[38;5;241m=\u001B[39m NimLogic\u001B[38;5;241m.\u001B[39mheuristic_evaluation(state, player, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmisere)\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m heuristic_score, \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     22\u001B[0m actions \u001B[38;5;241m=\u001B[39m NimLogic\u001B[38;5;241m.\u001B[39mavailable_actions(state)\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/nim/NimLogic.py:33\u001B[0m, in \u001B[0;36mNimLogic.heuristic_evaluation\u001B[0;34m(state, player, misere)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mheuristic_evaluation\u001B[39m(state, player, misere):\n\u001B[1;32m     32\u001B[0m     one_piles \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m pile \u001B[38;5;129;01min\u001B[39;00m state \u001B[38;5;28;01mif\u001B[39;00m pile \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 33\u001B[0m     greater_piles \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msum\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpile\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpile\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m greater_piles \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (one_piles \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m misere) \u001B[38;5;129;01mor\u001B[39;00m (one_piles \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m misere):\n",
      "File \u001B[0;32m~/PycharmProjects/reinforcement_learning_nim/nim/NimLogic.py:33\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[38;5;129m@staticmethod\u001B[39m\n\u001B[1;32m     31\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mheuristic_evaluation\u001B[39m(state, player, misere):\n\u001B[1;32m     32\u001B[0m     one_piles \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m pile \u001B[38;5;129;01min\u001B[39;00m state \u001B[38;5;28;01mif\u001B[39;00m pile \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 33\u001B[0m     greater_piles \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m pile \u001B[38;5;129;01min\u001B[39;00m state \u001B[38;5;28;01mif\u001B[39;00m pile \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     35\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m greater_piles \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     36\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m (one_piles \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m misere) \u001B[38;5;129;01mor\u001B[39;00m (one_piles \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m misere):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 66
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
